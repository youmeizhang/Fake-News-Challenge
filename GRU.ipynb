{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "valid = pd.read_csv('validation_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "body = pd.read_csv('article_body_texts.csv')\n",
    "train['class'] = train['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "valid['class'] = valid['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "\n",
    "body['articleBody'] = body['articleBody'].str.lower()\n",
    "test['Headline'] = test['Headline'].str.lower()\n",
    "valid['Headline'] = valid['Headline'].str.lower()\n",
    "train['Headline'] = train['Headline'].str.lower()\n",
    "\n",
    "\n",
    "body['articleBody'] = body['articleBody'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "train['Headline'] = train['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "valid['Headline'] = valid['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "test['Headline'] = test['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "\n",
    "body[\"articleBody\"] = body['articleBody'].str.replace('[^\\w\\s]',' ')\n",
    "test[\"Headline\"] = test['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "valid[\"Headline\"] = valid['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "train[\"Headline\"] = train['Headline'].str.replace('[^\\w\\s]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pd.merge(train, body, on='Body ID')\n",
    "full_valid = pd.merge(valid, body, on = 'Body ID')\n",
    "full_test = pd.merge(test, body, on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/y2568zha/msci641/lib64/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "full_data = full_train.append([full_valid, full_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 56.1 ms, total: 11.8 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tokenize import word_tokenize\n",
    "question_list = list(full_data['Headline'])# + list(full_data['articleBody'])\n",
    "question_list = [' '.join(word_tokenize(q)[:17]) for q in question_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 373 ms, total: 2min 7s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "body_list = list(full_data[\"articleBody\"])\n",
    "body_list = [' '.join(word_tokenize(q)[:200]) for q in body_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_list = question_list + body_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 19942\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=30000, filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n') \n",
    "tokenizer.fit_on_texts(whole_list)\n",
    "\n",
    "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k: v for k, v in tokenizer.word_index.items() if v < 30000}\n",
    "idx_to_word = dict((v,k) for k,v in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(whole_list)\n",
    "X = pad_sequences(X, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_headline = X[:len(X)//2]\n",
    "X_article = X[len(X)//2:]\n",
    "\n",
    "X_train_headline = X_headline[:len(full_train)]\n",
    "X_valid_headline = X_headline[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_headline = X_headline[len(full_train) + len(full_valid):]\n",
    "\n",
    "X_train_article = X_article[:len(full_train)]\n",
    "X_valid_article = X_article[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_article = X_article[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings = {}\n",
    "EMB_DIR = 'glove.6B.300d.txt.word2vec'\n",
    "f = open(EMB_DIR)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(word_index)+1, 300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('validation_data_2.csv')\n",
    "extra_features = pd.read_csv('full_stances.csv')\n",
    "validation_extra = valid_data[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()\n",
    "training_extra = extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extra_features = pd.read_csv(\"test_validation.csv\")\n",
    "test_extra = test_extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Y = pd.get_dummies(full_data['class']).values\n",
    "\n",
    "Y_train = Y[:len(full_train)]\n",
    "Y_valid = Y[len(full_train):len(full_train) + len(full_valid)]\n",
    "Y_test = Y[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_DIM = 64\n",
    "EMBEDDING_DIM = 300\n",
    "from keras.layers import *\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "otherInp = Input(shape = (4, ), name='extra_features')\n",
    "\n",
    "article_input = Input(shape=(207, ), name='article_input')\n",
    "embed = Embedding(input_dim=19943,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          trainable=False, name='word_embedding_layer1', \n",
    "                          mask_zero=True)\n",
    "article_input_1 = embed(article_input)\n",
    "first_lstm = Bidirectional(GRU(LSTM_DIM, return_state = True, name='lstm_layer11', dropout=0.3, recurrent_dropout=0.3))\n",
    "article_outputs, forward_h, backward_h = first_lstm(article_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_input = Input(shape=(207, ), name='headline_input')\n",
    "embed = Embedding(input_dim=19943,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          trainable=False, name='word_embedding_layer2', \n",
    "                          mask_zero=True)\n",
    "headline_input_2 = embed(headline_input)\n",
    "second_lstm = Bidirectional(GRU(LSTM_DIM, return_state = False, name='lstm_layer12', dropout=0.3, recurrent_dropout=0.3))\n",
    "state_h = [forward_h, backward_h]\n",
    "dec_outputs = second_lstm(headline_input_2, initial_state = state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedOut = Concatenate()([article_outputs, dec_outputs, otherInp])\n",
    "mergedOut = BatchNormalization()(mergedOut)\n",
    "\n",
    "mergedOut = Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.0015))(mergedOut)\n",
    "mergedOut = Dropout(0.25)(mergedOut)\n",
    "mergedOut = BatchNormalization()(mergedOut)\n",
    "\n",
    "mergedOut = Dense(4, activation='softmax', kernel_regularizer = regularizers.l2(0.0015))(mergedOut)\n",
    "gru = Model([article_input, headline_input, otherInp], mergedOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66677 samples, validate on 2438 samples\n",
      "Epoch 1/3\n",
      "66677/66677 [==============================] - 247s 4ms/step - loss: 0.5161 - categorical_accuracy: 0.8554 - val_loss: 0.3890 - val_categorical_accuracy: 0.8876\n",
      "Epoch 2/3\n",
      "66677/66677 [==============================] - 246s 4ms/step - loss: 0.2965 - categorical_accuracy: 0.9121 - val_loss: 0.3304 - val_categorical_accuracy: 0.8970\n",
      "Epoch 3/3\n",
      "66677/66677 [==============================] - 246s 4ms/step - loss: 0.2518 - categorical_accuracy: 0.9216 - val_loss: 0.3179 - val_categorical_accuracy: 0.8929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb63773b978>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "gru.fit(x = [X_train_article, X_train_headline, training_extra], \n",
    "          y = Y_train, \n",
    "          shuffle = True,\n",
    "          batch_size=64, \n",
    "          epochs=3,\n",
    "          validation_data=([X_valid_article, X_valid_headline, validation_extra], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66677 samples, validate on 2438 samples\n",
      "Epoch 1/1\n",
      "66677/66677 [==============================] - 245s 4ms/step - loss: 0.2340 - categorical_accuracy: 0.9261 - val_loss: 0.3188 - val_categorical_accuracy: 0.8925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6343ff518>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.fit(x = [X_train_article, X_train_headline, training_extra], \n",
    "          y = Y_train, \n",
    "          shuffle = True,\n",
    "          batch_size=64, \n",
    "          epochs=1,\n",
    "          validation_data=([X_valid_article, X_valid_headline, validation_extra], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_valid = gru.predict([X_valid_headline, X_valid_article, validation_extra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Y_valid_predict = []\n",
    "for i in range(len(result_valid)):\n",
    "    p = max(result_valid[i])\n",
    "    Y_valid_predict.append(list(result_valid[i]).index(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = full_data['class'].values\n",
    "Y_valid_real = Y[len(full_train):len(full_train) + len(full_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773584905660378"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 1745, 2: 564, 0: 118, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(Y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3.0: 1746, 2.0: 476, 0.0: 142, 1.0: 74})\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(Y_valid_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8143553389455028"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(Y_valid_predict, Y_valid_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  57,    3,   70,   12],\n",
       "       [  14,    0,   47,   13],\n",
       "       [  44,    6,  394,   32],\n",
       "       [   3,    2,   53, 1688]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43846154, 0.        , 0.75769231, 0.96705815])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(Y_valid_real, Y_valid_predict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5408029989203006"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_valid_real, Y_valid_predict, average = \"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_DIM = 64\n",
    "EMBEDDING_DIM = 300\n",
    "from keras.layers import *\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "import keras.regularizers\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "m1 = Sequential(layers=[\n",
    "    Embedding(input_dim=19943,  \n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer1', \n",
    "                          mask_zero=True),\n",
    "    Bidirectional(LSTM(LSTM_DIM, return_sequences=False, return_state = False, name='lstm_layer11')),\n",
    "    Dropout(rate=0.2, name='Headline_dropout1')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Sequential(layers=[\n",
    "    Embedding(input_dim=19943,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer2', \n",
    "                          mask_zero=True),\n",
    "    Bidirectional(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer21')),\n",
    "    Dropout(rate=0.2, name='Body_dropout1')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import * \n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate\n",
    "mergedOut = Concatenate()([m1.output, m2.output])\n",
    "\n",
    "mergedOut = Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.001))(mergedOut)\n",
    "mergedOut = Dense(4, activation='softmax', kernel_regularizer = regularizers.l2(0.001))(mergedOut)\n",
    "lstm = Model([m1.input, m2.input], mergedOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_embedding_layer1_input (In (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer2_input (In (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer1 (Embeddin (None, None, 300)    5982900     word_embedding_layer1_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer2 (Embeddin (None, None, 300)    5982900     word_embedding_layer2_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 128)          186880      word_embedding_layer1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 128)          186880      word_embedding_layer2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Headline_dropout1 (Dropout)     (None, 128)          0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Body_dropout1 (Dropout)         (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           Headline_dropout1[0][0]          \n",
      "                                                                 Body_dropout1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           8224        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            132         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,347,916\n",
      "Trainable params: 382,116\n",
      "Non-trainable params: 11,965,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
