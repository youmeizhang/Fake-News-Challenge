{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "valid = pd.read_csv('validation_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "body = pd.read_csv('article_body_texts.csv')\n",
    "train['class'] = train['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "valid['class'] = valid['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "\n",
    "body['articleBody'] = body['articleBody'].str.lower()\n",
    "test['Headline'] = test['Headline'].str.lower()\n",
    "valid['Headline'] = valid['Headline'].str.lower()\n",
    "train['Headline'] = train['Headline'].str.lower()\n",
    "\n",
    "\n",
    "body['articleBody'] = body['articleBody'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "train['Headline'] = train['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "valid['Headline'] = valid['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "test['Headline'] = test['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "\n",
    "body[\"articleBody\"] = body['articleBody'].str.replace('[^\\w\\s]',' ')\n",
    "test[\"Headline\"] = test['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "valid[\"Headline\"] = valid['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "train[\"Headline\"] = train['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "\n",
    "full_train = pd.merge(train, body, on='Body ID')\n",
    "full_valid = pd.merge(valid, body, on = 'Body ID')\n",
    "full_test = pd.merge(test, body, on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/y2568zha/msci641/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "full_data = full_train.append([full_valid, full_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 39 ms, total: 10.7 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tokenize import word_tokenize\n",
    "question_list = list(full_data['Headline'])# + list(full_data['articleBody'])\n",
    "question_list = [' '.join(word_tokenize(q)[:17]) for q in question_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 231 ms, total: 2min 4s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "body_list = list(full_data[\"articleBody\"])\n",
    "body_list = [' '.join(word_tokenize(q)[:685]) for q in body_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_list = question_list + body_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 27451\n"
     ]
    }
   ],
   "source": [
    "# Filters - removed '?' \n",
    "tokenizer = Tokenizer(num_words=30000, filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n') \n",
    "tokenizer.fit_on_texts(whole_list)\n",
    "\n",
    "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k: v for k, v in tokenizer.word_index.items() if v < 30000}\n",
    "idx_to_word = dict((v,k) for k,v in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(whole_list)\n",
    "X = pad_sequences(X, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_headline = X[:len(X)//2]\n",
    "X_article = X[len(X)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headline = X_headline[:len(full_train)]\n",
    "X_valid_headline = X_headline[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_headline = X_headline[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_article = X_article[:len(full_train)]\n",
    "X_valid_article = X_article[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_article = X_article[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66677, 695)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_article.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings = {}\n",
    "EMB_DIR = 'glove.6B.300d.txt.word2vec'\n",
    "f = open(EMB_DIR)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_matrix = np.random.uniform(-0.25, 0.25, size=(len(word_index)+1, 300))\n",
    "\n",
    "for word, i in word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = pd.read_csv(\"full_stances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('validation_data_2.csv')\n",
    "validation_extra = valid_data[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()\n",
    "training_extra = extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extra_features = pd.read_csv(\"test_data_preprocessing.csv\")\n",
    "test_extra = test_extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Y = pd.get_dummies(full_data['class']).values\n",
    "\n",
    "Y_train = Y[:len(full_train)]\n",
    "Y_valid = Y[len(full_train):len(full_train) + len(full_valid)]\n",
    "Y_test = Y[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional, Concatenate, concatenate, Dropout, Input, TimeDistributed, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "emb_look_up = Embedding(input_dim=(27452),\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        weights = [embeddings_matrix], \n",
    "                        trainable=False, \n",
    "                        mask_zero=False,\n",
    "                        name='q_embedding_lookup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_title (InputLayer)        (None, 695)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q_embedding_lookup (Embedding)  (None, 695, 300)     8235600     input_title[0][0]                \n",
      "                                                                 input_body[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 695, 128)     115328      q_embedding_lookup[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_body (InputLayer)         (None, 695)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 695, 128)     512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 695, 128)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 695, 128)     115328      q_embedding_lookup[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 232, 128)     0           dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 695, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 232, 256)     98560       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 695, 128)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 232, 256)     1024        conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 232, 128)     0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 232, 256)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 232, 256)     98560       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 232, 384)     295296      dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 232, 256)     1024        conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 232, 384)     1536        conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 232, 256)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 232, 384)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 232, 384)     295296      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 232, 384)     1536        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 232, 384)     0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 384)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 384)          0           dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 384)          0           global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "abs_8 (Abs)                     [(None, 384), (None, 0           global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 768)          0           multiply_8[0][0]                 \n",
      "                                                                 abs_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          393728      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          131328      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256)          1024        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_extra (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 260)          0           batch_normalization_14[0][0]     \n",
      "                                                                 input_extra[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 4)            1044        concatenate_11[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 9,787,236\n",
      "Trainable params: 1,548,052\n",
      "Non-trainable params: 8,239,184\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "n_classes = 4\n",
    "MAX_SENT_LEN_HEADLINE = 695\n",
    "MAX_SENT_LEN_BODY = 695\n",
    "emb_size = EMBEDDING_DIM\n",
    "n_hidden = 128 \n",
    "width = 3\n",
    "\n",
    "#ConvH\n",
    "convh_0  = Convolution1D(filters = n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convh_1  = Convolution1D(filters = n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convh_2  = Convolution1D(filters = 2*n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convh_3  = Convolution1D(filters = 2*n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convh_4  = Convolution1D(filters = 3*n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "\n",
    "#ConvB\n",
    "convb_0  = Convolution1D(filters = n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convb_1  = Convolution1D(filters = n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convb_2  = Convolution1D(filters = 2*n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convb_3  = Convolution1D(filters = 2*n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "convb_4  = Convolution1D(filters = 3*n_hidden, kernel_size = width, activation='relu', padding='same')\n",
    "\n",
    "#Dense\n",
    "dense_0 = Dense(n_hidden*4, activation='relu')\n",
    "dense_1 = Dense(n_hidden*4, activation='relu')\n",
    "dense_2 = Dense(n_hidden*2, activation='relu')\n",
    "dense_f = Dense(n_classes, activation='softmax', name='out')\n",
    "\n",
    "#Dropout\n",
    "dperc = 0.5\n",
    "\n",
    "#DropH\n",
    "droph_0 = Dropout(dperc)\n",
    "droph_1 = Dropout(dperc)\n",
    "droph_2 = Dropout(dperc)\n",
    "droph_3 = Dropout(dperc)\n",
    "droph_4 = Dropout(dperc)\n",
    "\n",
    "#DropB\n",
    "dropb_0 = Dropout(dperc)\n",
    "dropb_1 = Dropout(dperc)\n",
    "dropb_2 = Dropout(dperc)\n",
    "dropb_3 = Dropout(dperc)\n",
    "dropb_4 = Dropout(dperc)\n",
    "\n",
    "#PoolingH\n",
    "poolh_0 = MaxPooling1D(pool_size=3, padding='same')\n",
    "poolh_1 = MaxPooling1D(pool_size=3, padding='same')\n",
    "poolh_2 = MaxPooling1D(pool_size=3, padding='same')\n",
    "\n",
    "#PoolingB\n",
    "poolb_0 = MaxPooling1D(pool_size=3, padding='same')\n",
    "poolb_1 = MaxPooling1D(pool_size=3, padding='same')\n",
    "poolb_2 = MaxPooling1D(pool_size=3, padding='same')\n",
    "\n",
    "#Input formats\n",
    "input_title   = Input(shape=(MAX_SENT_LEN_HEADLINE, ), dtype='int32', name='input_title')\n",
    "input_body   = Input(shape=(MAX_SENT_LEN_BODY, ), dtype='int32', name='input_body')\n",
    "\n",
    "otherInp = Input(shape = (4,), dtype='float32', name='input_extra')\n",
    "\n",
    "#Create Layers - Title\n",
    "x = input_title\n",
    "x = emb_look_up(x)\n",
    "x = convh_0(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = droph_0(x)\n",
    "x = poolh_0(x)\n",
    "\n",
    "x = convh_3(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = droph_3(x)\n",
    "\n",
    "x = convh_4(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = droph_4(x)\n",
    "\n",
    "\n",
    "y = input_body\n",
    "y = emb_look_up(y)\n",
    "y = convb_0(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = dropb_0(y)\n",
    "y = poolb_0(y)\n",
    "\n",
    "y = convb_3(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = dropb_3(y)\n",
    "\n",
    "y = convb_4(y)\n",
    "x = BatchNormalization()(x)\n",
    "y = dropb_4(y)\n",
    "\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "y = GlobalMaxPooling1D()(y)\n",
    "\n",
    "\n",
    "z = concatenate([x, y])\n",
    "z = dense_0(z)\n",
    "\n",
    "z = dense_2(z)\n",
    "z = BatchNormalization()(z)\n",
    "\n",
    "z = concatenate([z, otherInp])\n",
    "out = dense_f(z)\n",
    "\n",
    "model = Model(inputs=[input_title, input_body, otherInp], outputs=[out])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after training around 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66677 samples, validate on 2438 samples\n",
      "Epoch 1/1\n",
      "66677/66677 [==============================] - 2153s 32ms/step - loss: 0.0508 - categorical_accuracy: 0.9832 - val_loss: 0.2617 - val_categorical_accuracy: 0.9143\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(x = [X_train_headline, X_train_article, training_extra], \n",
    "          y = Y_train, \n",
    "          shuffle = True,\n",
    "          batch_size=64,\n",
    "          epochs=1, \n",
    "          validation_data=([X_valid_headline, X_valid_article, validation_extra], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_valid = model.predict([X_valid_headline, X_valid_article, validation_extra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_accuracy(predict_result, y_test):\n",
    "    count = 0\n",
    "    for i in range(0, len(predict_result)):\n",
    "        if predict_result[i] == y_test[i] and y_test[i] == 3:\n",
    "            count = count + 0.25\n",
    "        elif y_test[i] != 3 and predict_result[i] != 3 and y_test[i] == predict_result[i]:\n",
    "            count = count + 1\n",
    "        elif y_test[i] != 3 and predict_result[i] != 3 and y_test[i] != predict_result[i]:\n",
    "            count += 0.25\n",
    "    a1 = Counter(y_test)\n",
    "    total_score = a1[3] * 0.25 + (a1[0] + a1[1] + a1[2]) * 1\n",
    "    accuracy = count / total_score\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Y_valid_predict = []\n",
    "for i in range(len(result_valid)):\n",
    "    p = max(result_valid[i])\n",
    "    Y_valid_predict.append(list(result_valid[i]).index(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = full_data['class'].values\n",
    "Y_valid_real = Y[len(full_train):len(full_train) + len(full_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.911402789171452"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 1660, 2: 586, 0: 121, 1: 71})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(Y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3.0: 1746, 2.0: 476, 0.0: 142, 1.0: 74})\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(Y_valid_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965440850686752"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(Y_valid_predict, Y_valid_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  79,   18,   44,    1],\n",
       "       [  28,   36,    6,    4],\n",
       "       [  11,    5,  456,    4],\n",
       "       [   3,   12,   80, 1651]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60076046, 0.49655172, 0.85875706, 0.96946565])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(Y_valid_real, Y_valid_predict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7313837228533875"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_valid_real, Y_valid_predict, average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model_json = model.to_json()\n",
    "with open(\"1D_CNN_8965.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"1D_CNN_8965.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
