{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "valid = pd.read_csv('validation_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "body = pd.read_csv('article_body_texts.csv')\n",
    "train['class'] = train['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "valid['class'] = valid['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "\n",
    "body['articleBody'] = body['articleBody'].str.lower()\n",
    "test['Headline'] = test['Headline'].str.lower()\n",
    "valid['Headline'] = valid['Headline'].str.lower()\n",
    "train['Headline'] = train['Headline'].str.lower()\n",
    "\n",
    "\n",
    "body['articleBody'] = body['articleBody'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "train['Headline'] = train['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "valid['Headline'] = valid['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "test['Headline'] = test['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "\n",
    "body[\"articleBody\"] = body['articleBody'].str.replace('[^\\w\\s]',' ')\n",
    "test[\"Headline\"] = test['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "valid[\"Headline\"] = valid['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "train[\"Headline\"] = train['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "\n",
    "full_train = pd.merge(train, body, on='Body ID')\n",
    "full_valid = pd.merge(valid, body, on = 'Body ID')\n",
    "full_test = pd.merge(test, body, on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/y2568zha/msci641/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "full_data = full_train.append([full_valid, full_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 39 ms, total: 10.7 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tokenize import word_tokenize\n",
    "question_list = list(full_data['Headline'])# + list(full_data['articleBody'])\n",
    "question_list = [' '.join(word_tokenize(q)[:17]) for q in question_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 231 ms, total: 2min 4s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "body_list = list(full_data[\"articleBody\"])\n",
    "body_list = [' '.join(word_tokenize(q)[:685]) for q in body_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_list = question_list + body_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 27451\n"
     ]
    }
   ],
   "source": [
    "# Filters - removed '?' \n",
    "tokenizer = Tokenizer(num_words=30000, filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n') \n",
    "tokenizer.fit_on_texts(whole_list)\n",
    "\n",
    "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k: v for k, v in tokenizer.word_index.items() if v < 30000}\n",
    "idx_to_word = dict((v,k) for k,v in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(whole_list)\n",
    "X = pad_sequences(X, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_headline = X[:len(X)//2]\n",
    "X_article = X[len(X)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headline = X_headline[:len(full_train)]\n",
    "X_valid_headline = X_headline[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_headline = X_headline[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_article = X_article[:len(full_train)]\n",
    "X_valid_article = X_article[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_article = X_article[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66677, 695)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_article.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings = {}\n",
    "EMB_DIR = 'glove.6B.300d.txt.word2vec'\n",
    "f = open(EMB_DIR)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_matrix = np.random.uniform(-0.25, 0.25, size=(len(word_index)+1, 300))\n",
    "\n",
    "for word, i in word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = pd.read_csv(\"full_stances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('validation_data_2.csv')\n",
    "validation_extra = valid_data[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()\n",
    "training_extra = extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extra_features = pd.read_csv(\"test_data_preprocessing.csv\")\n",
    "test_extra = test_extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Y = pd.get_dummies(full_data['class']).values\n",
    "\n",
    "Y_train = Y[:len(full_train)]\n",
    "Y_valid = Y[len(full_train):len(full_train) + len(full_valid)]\n",
    "Y_test = Y[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional, Concatenate, concatenate, Dropout, Input, TimeDistributed, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "emb_look_up = Embedding(input_dim=(27452),\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        weights = [embeddings_matrix], \n",
    "                        trainable=False, \n",
    "                        mask_zero=False,\n",
    "                        name='q_embedding_lookup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cnn with simple attention mechanism\n",
    "(credit to: https://github.com/GauravBh1010tt/DeepLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "import warnings\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import numpy as np\n",
    "import gensim as gen\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Layer,Lambda, Dropout, Activation, Input, Concatenate, Multiply\n",
    "from keras.layers import Embedding, BatchNormalization\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abs(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Abs, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.abs(x[0]- x[1])\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/y2568zha/msci641/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=100, kernel_size=4, strides=1, padding=\"valid\")`\n",
      "/home/students/y2568zha/msci641/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=100, kernel_size=4, strides=1, padding=\"valid\")`\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "LSTM_neurons = 64 \n",
    "dense_neuron = 32 \n",
    "dimx = 695\n",
    "dimy = 695\n",
    "lamda = 0.0\n",
    "nb_filter = 100\n",
    "filter_length = 4\n",
    "vocab_size = 10000\n",
    "batch_size = 64 \n",
    "epochs = 3\n",
    "ntn_out = 16\n",
    "ntn_in = nb_filter \n",
    "state = False\n",
    "    \n",
    "inpx   = Input(shape=(dimx, ), dtype='int32', name='input_title')\n",
    "inpy   = Input(shape=(dimy, ), dtype='int32', name='input_body')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "emb_look_up = Embedding(input_dim=(27452),\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        weights = [embeddings_matrix], \n",
    "                        trainable=False, \n",
    "                        mask_zero=False,\n",
    "                        name='q_embedding_lookup')\n",
    "x = inpx\n",
    "x = emb_look_up(x)\n",
    "    \n",
    "y = inpy\n",
    "y = emb_look_up(y)\n",
    "    \n",
    "ques = Convolution1D(nb_filter=nb_filter, filter_length=filter_length,\n",
    "                         border_mode='valid', activation='relu',\n",
    "                         subsample_length=1)(x)\n",
    "\n",
    "ques = BatchNormalization()(ques)\n",
    "                            \n",
    "ans = Convolution1D(nb_filter=nb_filter, filter_length=filter_length,\n",
    "                        border_mode='valid', activation='relu',\n",
    "                        subsample_length=1)(y)\n",
    "\n",
    "ans = BatchNormalization()(ans)\n",
    "\n",
    "hx = GlobalMaxPooling1D()(ques)\n",
    "hy = GlobalMaxPooling1D()(ans)\n",
    "\n",
    "    \n",
    "h1 = Multiply()([hx,hy])\n",
    "h2 = Abs()([hx,hy])\n",
    "h = Concatenate()([h1, h2])\n",
    "\n",
    "h = Dense(dense_neuron, activation='relu',name='wrap')(h)\n",
    "h = BatchNormalization()(h)\n",
    "\n",
    "score = Dense(4,activation='softmax',name='score')(h)\n",
    "model = Model( [inpx, inpy, otherInp],score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_title (InputLayer)        (None, 695)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_body (InputLayer)         (None, 695)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q_embedding_lookup (Embedding)  (None, 695, 300)     8235600     input_title[0][0]                \n",
      "                                                                 input_body[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 692, 100)     120100      q_embedding_lookup[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 692, 100)     120100      q_embedding_lookup[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 692, 100)     400         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 692, 100)     400         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 100)          0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "abs_2 (Abs)                     [(None, 100), (None, 0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200)          0           multiply_2[0][0]                 \n",
      "                                                                 abs_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "wrap (Dense)                    (None, 32)           6432        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32)           128         wrap[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "score (Dense)                   (None, 4)            132         batch_normalization_7[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 8,483,292\n",
      "Trainable params: 247,228\n",
      "Non-trainable params: 8,236,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66677 samples, validate on 2438 samples\n",
      "Epoch 1/2\n",
      "66677/66677 [==============================] - 584s 9ms/step - loss: 0.0890 - categorical_accuracy: 0.9675 - val_loss: 0.5500 - val_categorical_accuracy: 0.8425\n",
      "Epoch 2/2\n",
      "66677/66677 [==============================] - 581s 9ms/step - loss: 0.0656 - categorical_accuracy: 0.9764 - val_loss: 0.5149 - val_categorical_accuracy: 0.8679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0e82a95198>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [X_train_headline, X_train_article, training_extra], \n",
    "          y = Y_train, \n",
    "          shuffle = True,\n",
    "          batch_size=64, \n",
    "          epochs=2,\n",
    "          #verbose=2,\n",
    "          validation_data=([X_valid_headline, X_valid_article, validation_extra], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_accuracy(predict_result, y_test):\n",
    "    count = 0\n",
    "    for i in range(0, len(predict_result)):\n",
    "        if predict_result[i] == y_test[i] and y_test[i] == 3:\n",
    "            count = count + 0.25\n",
    "        elif y_test[i] != 3 and predict_result[i] != 3 and y_test[i] == predict_result[i]:\n",
    "            count = count + 1\n",
    "        elif y_test[i] != 3 and predict_result[i] != 3 and y_test[i] != predict_result[i]:\n",
    "            count += 0.25\n",
    "    a1 = Counter(y_test)\n",
    "    total_score = a1[3] * 0.25 + (a1[0] + a1[1] + a1[2]) * 1\n",
    "    accuracy = count / total_score\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_valid = model.predict([X_valid_headline, X_valid_article, validation_extra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Y_valid_predict = []\n",
    "for i in range(len(result_valid)):\n",
    "    p = max(result_valid[i])\n",
    "    Y_valid_predict.append(list(result_valid[i]).index(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = full_data['class'].values\n",
    "Y_valid_real = Y[len(full_train):len(full_train) + len(full_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8679245283018868"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 1806, 2: 446, 0: 118, 1: 68})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(Y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3.0: 1746, 2.0: 476, 0.0: 142, 1.0: 74})\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(Y_valid_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7899867080194949"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(Y_valid_predict, Y_valid_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  68,   16,   26,   32],\n",
       "       [  13,   27,   11,   23],\n",
       "       [  18,   22,  353,   83],\n",
       "       [  19,    3,   56, 1668]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52307692, 0.38028169, 0.76572668, 0.93918919])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(Y_valid_real, Y_valid_predict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.652068620883735"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_valid_real, Y_valid_predict, average = \"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-directional LSTM with simple attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "LSTM_neurons = 50\n",
    "dense_neuron = 16\n",
    "lamda = 0.0\n",
    "nb_filter = 100\n",
    "filter_length = 4\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "ntn_out = 16\n",
    "ntn_in = nb_filter \n",
    "state = False\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(LSTM_neurons,return_sequences=True),merge_mode='sum')   \n",
    "hx = shared_lstm(x)\n",
    "hy = shared_lstm(y)\n",
    "\n",
    "h1 = Flatten()(hx)\n",
    "h2 = Flatten()(hy)\n",
    "\n",
    "hx1 = Multiply()([h1,h2])\n",
    "hx2 = Abs()([h1,h2])\n",
    "\n",
    "h = Concatenate()([hx1, hx2])\n",
    "\n",
    "wrap = Dense(dense_neuron, activation='relu',name='wrap')(h)\n",
    "score = Dense(4,activation='softmax',name='score')(wrap)\n",
    "model = Model( [inpx,inpy],score)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile( loss='categorical_crossentropy',optimizer=\"adadelta\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66677 samples, validate on 2438 samples\n",
      "Epoch 1/1\n",
      "66677/66677 [==============================] - 668s 10ms/step - loss: 0.1922 - acc: 0.9272 - val_loss: 0.4186 - val_acc: 0.8749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabbb882400>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x = [X_train_headline, X_train_article], \n",
    "          y = Y_train, \n",
    "          shuffle = True,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=([X_valid_headline, X_valid_article], Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
