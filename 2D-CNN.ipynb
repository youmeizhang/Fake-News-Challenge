{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "valid = pd.read_csv('validation_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "body = pd.read_csv('article_body_texts.csv')\n",
    "train['class'] = train['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "valid['class'] = valid['Stance'].map({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3})\n",
    "\n",
    "body['articleBody'] = body['articleBody'].str.lower()\n",
    "test['Headline'] = test['Headline'].str.lower()\n",
    "valid['Headline'] = valid['Headline'].str.lower()\n",
    "train['Headline'] = train['Headline'].str.lower()\n",
    "\n",
    "\n",
    "body['articleBody'] = body['articleBody'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "train['Headline'] = train['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "valid['Headline'] = valid['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "test['Headline'] = test['Headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\\\s]',' ',x)))\n",
    "\n",
    "body[\"articleBody\"] = body['articleBody'].str.replace('[^\\w\\s]',' ')\n",
    "test[\"Headline\"] = test['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "valid[\"Headline\"] = valid['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "train[\"Headline\"] = train['Headline'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "\n",
    "full_train = pd.merge(train, body, on='Body ID')\n",
    "full_valid = pd.merge(valid, body, on = 'Body ID')\n",
    "full_test = pd.merge(test, body, on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/y2568zha/msci641/lib64/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "full_data = full_train.append([full_valid, full_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile Sentence Length: 17.0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "import numpy as np\n",
    "word_seq = [text_to_word_sequence(sent) for sent in full_data['Headline']]\n",
    "print('90th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq], 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile Sentence Length: 685.0\n"
     ]
    }
   ],
   "source": [
    "word_seq2 = [text_to_word_sequence(sent) for sent in full_data['articleBody']]\n",
    "print('90th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq2], 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.24 s, sys: 220 ms, total: 7.46 s\n",
      "Wall time: 9.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tokenize import word_tokenize\n",
    "question_list = list(full_data['Headline'])\n",
    "question_list = [' '.join(word_tokenize(q)[:17]) for q in question_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 55.6 ms, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "body_list = list(full_data[\"articleBody\"])\n",
    "body_list = [' '.join(word_tokenize(q)[:685]) for q in body_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_list = question_list + body_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 27451\n"
     ]
    }
   ],
   "source": [
    "# Filters - removed '?' \n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n') \n",
    "tokenizer.fit_on_texts(whole_list)\n",
    "\n",
    "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k: v for k, v in tokenizer.word_index.items() if v < 30000}\n",
    "idx_to_word = dict((v, k) for k,v in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(whole_list)\n",
    "X = pad_sequences(X, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_headline = X[:len(X)//2]\n",
    "X_article = X[len(X)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headline = X_headline[:len(full_train)]\n",
    "X_valid_headline = X_headline[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_headline = X_headline[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_article = X_article[:len(full_train)]\n",
    "X_valid_article = X_article[len(full_train):len(full_train) + len(full_valid)]\n",
    "X_test_article = X_article[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "EMB_DIR = 'glove.6B.300d.txt.word2vec'\n",
    "f = open(EMB_DIR)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(word_index)+1, 300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = pd.read_csv(\"full_stances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('validation_data_2.csv')\n",
    "validation_extra = valid_data[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()\n",
    "training_extra = extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extra_features = pd.read_csv(\"test_data_preprocessing.csv\")\n",
    "test_extra = test_extra_features[['bigram_ratio', 'unigram_ratio', 'trigram_ratio', 'tfidf_similarity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Y = pd.get_dummies(full_data['class']).values\n",
    "\n",
    "Y_train = Y[:len(full_train)]\n",
    "Y_valid = Y[len(full_train):len(full_train) + len(full_valid)]\n",
    "Y_test = Y[len(full_train) + len(full_valid):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, Dense, Concatenate, Conv2D, Reshape, MaxPooling1D, Flatten, BatchNormalization, Activation, Dropout, Embedding\n",
    "\n",
    "bi_filter_size = 2\n",
    "tri_filter_size = 3\n",
    "\n",
    "num_filters = 40\n",
    "\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "MAX_SENT_LEN_HEADLINE = 695\n",
    "MAX_SENT_LEN_BODY = 695\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credit to: kathihareesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(MAX_SENT_LEN_HEADLINE, ), name='q1_input')\n",
    "otherInp = Input(shape = (4, ), name='extra_features')\n",
    "\n",
    "emb_look_up = Embedding(input_dim=(27452),\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        weights = [embeddings_matrix], \n",
    "                        trainable=False, \n",
    "                        mask_zero=False,\n",
    "                        name='q_embedding_lookup')\n",
    "\n",
    "emb_1 = emb_look_up(input_1)\n",
    "\n",
    "emb_1 = Reshape(target_shape=(1, MAX_SENT_LEN_HEADLINE, EMBEDDING_DIM), \n",
    "                name='q1_embedding_reshape')(emb_1) \n",
    "\n",
    "conv_1_bi =  Conv2D(filters=num_filters, \n",
    "                    kernel_size=(bi_filter_size, EMBEDDING_DIM), \n",
    "                    padding='valid', \n",
    "                    activation='relu', \n",
    "                    data_format='channels_first', \n",
    "                    name='q1_bigram_conv')(emb_1)\n",
    "\n",
    "conv_1_tri =  Conv2D(filters=num_filters, \n",
    "                     kernel_size=(tri_filter_size, EMBEDDING_DIM), \n",
    "                     padding='valid', \n",
    "                     activation='relu', \n",
    "                     data_format='channels_first', \n",
    "                     name='q1_trigram_conv')(emb_1)\n",
    "\n",
    "bi_out_timesteps = MAX_SENT_LEN_HEADLINE - bi_filter_size + 1 \n",
    "tri_out_timesteps = MAX_SENT_LEN_HEADLINE - tri_filter_size + 1\n",
    "\n",
    "conv_1_bi = Reshape(target_shape=(bi_out_timesteps, num_filters), \n",
    "                    name='q1_bigram_conv_reshape')(conv_1_bi) \n",
    "conv_1_tri = Reshape(target_shape=(tri_out_timesteps, num_filters),\n",
    "                     name='q1_trigram_conv_reshape')(conv_1_tri)\n",
    "\n",
    "max_pool_1_bi = MaxPooling1D(pool_size = bi_out_timesteps,\n",
    "                             name='q1_bigram_maxpool')(conv_1_bi)\n",
    "max_pool_1_tri = MaxPooling1D(pool_size = tri_out_timesteps,\n",
    "                              name='q1_trigram_maxpool')(conv_1_tri)\n",
    "\n",
    "merged_1 = Concatenate(name='q1_maxpool_concat')([max_pool_1_bi, max_pool_1_tri])\n",
    "\n",
    "dropout_1 = Dropout(rate=0.2, \n",
    "                    name='q1_dropout')(merged_1)\n",
    "flatten_1 = Flatten(name='q1_flatten')(dropout_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2 = Input(shape=(MAX_SENT_LEN_BODY, ), name='q2_input')\n",
    "\n",
    "emb_2 = emb_look_up(input_2)\n",
    "emb_2 = Reshape((1, MAX_SENT_LEN_BODY, EMBEDDING_DIM), \n",
    "                name='q2_embedding_reshape')(emb_2)\n",
    "\n",
    "conv_2_bi =  Conv2D(filters=num_filters, \n",
    "                    kernel_size=(bi_filter_size, EMBEDDING_DIM), \n",
    "                    padding='valid', \n",
    "                    activation='relu', \n",
    "                    data_format='channels_first', \n",
    "                    name='q2_bigram_conv')(emb_2)\n",
    "\n",
    "conv_2_tri =  Conv2D(filters=num_filters, \n",
    "                     kernel_size=(tri_filter_size, EMBEDDING_DIM), \n",
    "                     padding='valid', \n",
    "                     activation='relu', \n",
    "                     data_format='channels_first', \n",
    "                     name='q2_trigram_conv')(emb_2)\n",
    "\n",
    "bi_out_timesteps_2 = MAX_SENT_LEN_BODY - bi_filter_size + 1\n",
    "tri_out_timesteps_2 = MAX_SENT_LEN_BODY - tri_filter_size + 1 \n",
    "\n",
    "conv_2_bi = Reshape((bi_out_timesteps_2, num_filters), \n",
    "                    name='q2_bigram_conv_reshape')(conv_2_bi) \n",
    "conv_2_tri = Reshape((tri_out_timesteps_2, num_filters), \n",
    "                     name='q2_trigram_conv_reshape')(conv_2_tri)\n",
    "\n",
    "max_pool_2_bi = MaxPooling1D(pool_size = bi_out_timesteps_2, \n",
    "                             name='q2_bigram_maxpool')(conv_2_bi)\n",
    "max_pool_2_tri = MaxPooling1D(pool_size = tri_out_timesteps_2, \n",
    "                              name='q2_trigram_maxpool')(conv_2_tri)\n",
    "\n",
    "merged_2 = Concatenate(name='q2_maxpool_flatten')([max_pool_2_bi, max_pool_2_tri])\n",
    "dropout_2 = Dropout(rate=0.2, \n",
    "                    name='q2_dropout')(merged_2)\n",
    "flatten_2 = Flatten(name='q2_flatten')(dropout_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q1_input (InputLayer)           (None, 695)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q2_input (InputLayer)           (None, 695)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q_embedding_lookup (Embedding)  (None, 695, 300)     8235600     q1_input[0][0]                   \n",
      "                                                                 q2_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "q1_embedding_reshape (Reshape)  (None, 1, 695, 300)  0           q_embedding_lookup[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "q2_embedding_reshape (Reshape)  (None, 1, 695, 300)  0           q_embedding_lookup[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "q1_bigram_conv (Conv2D)         (None, 40, 694, 1)   24040       q1_embedding_reshape[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "q1_trigram_conv (Conv2D)        (None, 40, 693, 1)   36040       q1_embedding_reshape[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "q2_bigram_conv (Conv2D)         (None, 40, 694, 1)   24040       q2_embedding_reshape[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "q2_trigram_conv (Conv2D)        (None, 40, 693, 1)   36040       q2_embedding_reshape[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "q1_bigram_conv_reshape (Reshape (None, 694, 40)      0           q1_bigram_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "q1_trigram_conv_reshape (Reshap (None, 693, 40)      0           q1_trigram_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q2_bigram_conv_reshape (Reshape (None, 694, 40)      0           q2_bigram_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "q2_trigram_conv_reshape (Reshap (None, 693, 40)      0           q2_trigram_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q1_bigram_maxpool (MaxPooling1D (None, 1, 40)        0           q1_bigram_conv_reshape[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "q1_trigram_maxpool (MaxPooling1 (None, 1, 40)        0           q1_trigram_conv_reshape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "q2_bigram_maxpool (MaxPooling1D (None, 1, 40)        0           q2_bigram_conv_reshape[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "q2_trigram_maxpool (MaxPooling1 (None, 1, 40)        0           q2_trigram_conv_reshape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "q1_maxpool_concat (Concatenate) (None, 1, 80)        0           q1_bigram_maxpool[0][0]          \n",
      "                                                                 q1_trigram_maxpool[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "q2_maxpool_flatten (Concatenate (None, 1, 80)        0           q2_bigram_maxpool[0][0]          \n",
      "                                                                 q2_trigram_maxpool[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "q1_dropout (Dropout)            (None, 1, 80)        0           q1_maxpool_concat[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "q2_dropout (Dropout)            (None, 1, 80)        0           q2_maxpool_flatten[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "q1_flatten (Flatten)            (None, 80)           0           q1_dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "q2_flatten (Flatten)            (None, 80)           0           q2_dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "extra_features (InputLayer)     (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q1_q2_concat (Concatenate)      (None, 164)          0           q1_flatten[0][0]                 \n",
      "                                                                 q2_flatten[0][0]                 \n",
      "                                                                 extra_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "q1_q2_dense (Dense)             (None, 10)           1650        q1_q2_concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm (BatchNormalization)  (None, 10)           40          q1_q2_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "relu_activation (Activation)    (None, 10)           0           batchnorm[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_dropout (Dropout)         (None, 10)           0           relu_activation[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 4)            44          dense_dropout[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,357,494\n",
      "Trainable params: 121,874\n",
      "Non-trainable params: 8,235,620\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "merged = Concatenate(name='q1_q2_concat')([flatten_1, flatten_2, otherInp])\n",
    "dense_1 = Dense(units=10, \n",
    "                name='q1_q2_dense')(merged)\n",
    "bn_1 = BatchNormalization(name='batchnorm')(dense_1)\n",
    "relu_1 = Activation(activation='relu', \n",
    "                    name='relu_activation')(bn_1)\n",
    "dense_1_dropout = Dropout(0.2, \n",
    "                          name='dense_dropout')(relu_1)\n",
    "\n",
    "output_prob = Dense(units=4, \n",
    "                    activation='softmax', \n",
    "                    name='output_layer')(dense_1_dropout)\n",
    "model = Model(inputs=[input_1, input_2, otherInp], outputs=output_prob, name='text_pair_cnn')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66677 samples, validate on 2438 samples\n",
      "Epoch 1/5\n",
      "66677/66677 [==============================] - 333s 5ms/step - loss: 0.4915 - categorical_accuracy: 0.8157 - val_loss: 0.3052 - val_categorical_accuracy: 0.8888\n",
      "Epoch 2/5\n",
      "66677/66677 [==============================] - 332s 5ms/step - loss: 0.2762 - categorical_accuracy: 0.8985 - val_loss: 0.2779 - val_categorical_accuracy: 0.8991\n",
      "Epoch 3/5\n",
      "66677/66677 [==============================] - 341s 5ms/step - loss: 0.2294 - categorical_accuracy: 0.9202 - val_loss: 0.2595 - val_categorical_accuracy: 0.9016\n",
      "Epoch 4/5\n",
      "66677/66677 [==============================] - 341s 5ms/step - loss: 0.2005 - categorical_accuracy: 0.9291 - val_loss: 0.2680 - val_categorical_accuracy: 0.9036\n",
      "Epoch 5/5\n",
      "66677/66677 [==============================] - 340s 5ms/step - loss: 0.1843 - categorical_accuracy: 0.9341 - val_loss: 0.2633 - val_categorical_accuracy: 0.9003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2a60b9e3c8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.fit(x = [X_train_headline, X_train_article, training_extra], \n",
    "          y = Y_train, \n",
    "          shuffle = True,\n",
    "          batch_size=32, \n",
    "          epochs=5, \n",
    "          validation_data=([X_valid_headline, X_valid_article, validation_extra], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_accuracy_2(predict_result, y_test):\n",
    "    count = 0\n",
    "    for i in range(0, len(predict_result)):\n",
    "        if predict_result[i] == y_test[i] and y_test[i] == 3:\n",
    "            count += 0.25\n",
    "        elif y_test[i] != 3:\n",
    "            if predict_result[i] != 3:\n",
    "                count += 0.25\n",
    "                if predict_result[i] == y_test[i]:\n",
    "                    count += 0.75\n",
    "    a1 = Counter(y_test)\n",
    "    total_score = a1[3] * 0.25 + (a1[0] + a1[1] + a1[2]) * 1\n",
    "    accuracy = count / total_score\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_valid = model.predict([X_valid_headline, X_valid_article, validation_extra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_valid_predict = []\n",
    "for i in range(len(result_valid)):\n",
    "    p = max(result_valid[i])\n",
    "    Y_valid_predict.append(list(result_valid[i]).index(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = full_data['class'].values\n",
    "Y_valid_real = Y[len(full_train):len(full_train) + len(full_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9031993437243643"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 1753, 2: 489, 0: 194, 1: 2})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(Y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 1746, 2: 476, 0: 142, 1: 74})\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(valid['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8486929552503323"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weigted score after training around 5 - 10 epochs\n",
    "get_accuracy_2(Y_valid_predict, Y_valid_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  88,    0,   47,    7],\n",
       "       [  51,    1,   14,    8],\n",
       "       [  53,    0,  399,   24],\n",
       "       [   2,    1,   29, 1714]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_valid_real, Y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52380952, 0.02631579, 0.82694301, 0.97970849])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(Y_valid_real, Y_valid_predict, average=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5891942016510059"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_valid_real, Y_valid_predict, average='macro') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
